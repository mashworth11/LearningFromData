#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Wed Jan 30 09:50:59 2019

@author: markashworth
"""
"""
Program to be used to answer the non-linear regression questions (Q8-10) in 
Homework 2 of the Learning from Data course
"""
import random 
import numpy as np
import matplotlib.pyplot as plt

class generateNL_Labels(object):
    """
    Class that allows for the generation of non-linear labels
    """
    def __init__(self, N, generateNoise = False):
        """
        N is the sample size
        X is the Nx2 input coordinate vect0r
        Y is the output coordinate vector
        """
        self.N = N
        self.X = self.generateXMat()
        self.Y = self.evalPoints() # No noise added yet
        if generateNoise == True:
            self.makeSomeNoise()
        self.mappedVec = self.mapVector() # Dict of input data (keys) and labels (values)        
    
    def generateXMat(self):
        """
        Function used to generate an Nx2 matrix of (x1,x2) coordinate pairs.
        This will make the generate of noise far easier.
        """
        X = np.zeros((self.N, 2))
        for r in range(self.N):
            X[r] = [random.choice([-1, 1])*random.random(), random.choice([-1, 1])*random.random()]
        return X
    
    def evalPoints(self): # TARGET FUNCTION
        """
        Evaluate a single data point, x, given the linear equation splitting up
        the domain. This evalutaion is what constitutes the TARGET FUNCTION. 
        """
        sign = lambda x: x and (-1 if x < 0 else 1)
        return list(map(sign, self.X[:,0]**2 + self.X[:,1]**2 - 0.6))
    
    def makeSomeNoise(self):
        """
        Method that adds some noiseyness to your evaluated data.
        """
        noiseyRows = random.sample(range(self.N), int(self.N*0.1))
        for i in noiseyRows:
            self.Y[i] = self.Y[i]*-1
    
    def mapVector(self):
        """
        Evalutaion of N vector of input data points based on the target function. That is, 
        given some input data we want to map outputs (or labels) to them. The mapping
        is expressed as a dictionary with keys of input points and values of outputs.
        """
        return {tuple(self.X[i]):self.Y[i] for i in range(np.shape(self.X)[0])}
    
    def visualise(self):
        """
        Method to visual the training data. Blue circles denote a classification
        of +1 whilst red crosses denote a classifiation of -1. 
        """
        for i in range(self.N):
            if self.Y[i] > 0:
                plt.plot(self.X[i,0], self.X[i,1], 'bo')
            else:
                plt.plot(self.X[i,0], self.X[i,1], 'rx')            
 
              
class newLinearRegression(object):
    """
    Perform linear regression to find the target function. This amounts to 
    finding the optimal weights that minimise the in sample square error. This
    can be done exactly for the case of linear regression. 
    
    Note that here he have added to our linearRegression class the ability to add
    noise. 
    """
    def __init__(self, N):
        """
        X is the input data with the addition of x0 = 1
        y is the output data or labels
        w are the weights generated by the linear regression algorithm
        """
        self.N = N
        self.data = generateNL_Labels(self.N, generateNoise = False) # generate input/output data 
        self.X = np.array([[1]+list(i) for i in self.data.mappedVec.keys()])
        self.Y = np.array(list(self.data.mappedVec.values()))
        self.w = self.calcWeights()
        self.Ein = self.calcEin()
        
    def calcWeights(self):
        """
        Calculate the weights using the algorithm based on minimising the least
        squared error (involves calculating the psuedo-inverse matrix of X)
        """
        self.pinvX = np.dot(np.linalg.inv((np.dot(self.X.transpose(), self.X))), self.X.transpose())
        return np.dot(self.pinvX, self.Y)
        
    def calcEin(self):
        """
        Calculate the in sample error.
        """
        return (1/self.N)*(np.linalg.norm(np.dot(self.X, self.w) - self.Y, 2))**2
    
    def visualise(self):
        """
        Method to visual the training data. Blue circles denote a classification
        of +1 whilst red crosses denote a classifiation of -1. 
        """
        for i in range(self.N):
            if np.dot(self.X[i], self.w) > 0:
                plt.plot(self.X[i,1], self.X[i,2], 'go')
            else:
                plt.plot(self.X[i,1], self.X[i,2], 'kx')            
 
        
    
class nonLinearRegression(object):
    """
    Perform linear regression to find the target function. This amounts to 
    finding the optimal weights that minimise the in sample square error. This
    can be done exactly for the case of linear regression. 
    
    Note that here we have added to our linearRegression class the ability to add
    noise. 
    """
    def __init__(self, N):
        """
        X is the input data with the addition of x0 = 1
        y is the output data or labels
        w are the weights generated by the linear regression algorithm
        """
        self.N = N
        self.data = generateNL_Labels(self.N, generateNoise = True) # generate input/output data 
        self.X = np.array([[1] + [i[0], i[1], i[0]*i[1], i[0]**2, i[1]**2] \
                           for i in self.data.mappedVec.keys()])
        self.Y = np.array(list(self.data.mappedVec.values()))
        self.w = self.calcWeights()
        self.Ein = self.calcEin()
        
    def calcWeights(self):
        """
        Calculate the weights using the algorithm based on minimising the least
        squared error (involves calculating the psuedo-inverse matrix of X)
        """
        self.pinvX = np.linalg.pinv(self.X)
        return np.dot(self.pinvX, self.Y)
        
    def calcEin(self):
        """
        Calculate the in sample error.
        """
        return (1/self.N)*(np.linalg.norm(np.dot(self.X, self.w) - self.Y, 2))**2


# Question 8   
def simulateNewLinReg(N, I):
    """
    Simulate the linear regression problem. Want to generate a dictionary of 
    g functions : Ein.
    N is the sample size
    I is the number of experiments 
    """
    lRdict = {}
    for i in range(I):
        lR = newLinearRegression(N)
        lRdict.update({tuple(lR.w):lR.Ein})
    return lRdict    


# Question 9
def findWeightingVector(N):
    """
    Function to find the weighting vector as asked for in question 9 of ze homework
    2.
    """
    for i in range(100):
        nLR = nonLinearRegression(1000)
        if i == 0:
            weights = nLR.w
        else:
            weights += nLR.w
            
    return weights/100


# Question 10
def EoutOfSampleNL(N, w):
    """
    Estimate the average out of sample error over 1000 runs in which we generate
    a set of 1000 new points of training data. 
    N is the sample size (in this case 1000).
    """
    Eout = []
    for i in range(1000):
        newDat = generateNL_Labels(1000, generateNoise = True)
        X = np.array([[1] + [i[0], i[1], i[0]*i[1], i[0]**2, i[1]**2] \
                 for i in newDat.mappedVec.keys()])
        Y = np.array([y for y in newDat.mappedVec.values()])
        sign = lambda x: x and (-1 if x < 0 else 1)
        Eout.append((1/N)*(np.linalg.norm(list(map(sign,np.dot(X, np.array(w)))) - Y, 2))**2)
    return np.mean(Eout)
                 